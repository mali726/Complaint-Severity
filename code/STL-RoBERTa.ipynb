{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"STL-RoBERTa.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMtQnxnFOWDvKey1gTbJpGQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"8J1phE4TpUB4"},"source":["import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold\n","import numpy as np\n","from keras.preprocessing.sequence import pad_sequences\n","from transformers import AdamW\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel\n","from sklearn import metrics"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PoTOLBBXSQhn"},"source":["## Preprocess and Prepare Dataset\n","complaint_severity_data_4label.csv is extract from complaint_severity_data.csv where multilabel = {1, 2, 3, 4} (excluding non-complaints)"]},{"cell_type":"code","metadata":{"id":"L9fev_OlTJzq"},"source":["# Set the maximum sequence length\n","MAX_LEN = 50\n","\n","df = pd.read_csv('complaint_severity_data_4label.csv', header=None, names=['id', 'text', 'binarylabel', 'multilabel', 'domain'])\n","\n","# Create sentence and label list\n","sentences = df.text.values\n","labels_multi = df.multilabel.values\n","\n","sentences = ['[CLS] ' + sentence + ' [SEP]' for sentence in sentences]\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","# Use the RoBERTa tokenizer to convert the tokens to their index numbers in the RoBERTa vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# Pad input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')\n","\n","# Create attention masks\n","attention_masks = []\n","\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djgxt4X5TK3n"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"Dr-MevEmxJ62"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0EuChxvdTU_"},"source":["batch_size = 32\n","n_epoch = 20\n","\n","# Nested cross validation (outer-10; inner-3)\n","skf_outer = StratifiedKFold(n_splits=10, random_state=100)\n","skf_inner = StratifiedKFold(n_splits=3, random_state=100)\n","  \n","fold_outer = 1\n","fold_inner = 1\n","\n","# Acc/precision/recall/f1s over 10 folds\n","test_acc_10 = []\n","test_precision_10 = []\n","test_recall_10 = []\n","test_f1_10 = []\n","\n","# Outer loop\n","for train_index, test_index in skf_outer.split(input_ids, labels_multi):\n","    print('outter fold', fold_outer)\n","    x_train, x_test = np.array(input_ids)[train_index], np.array(input_ids)[test_index]\n","    y_train, y_test = np.array(labels_multi)[train_index], np.array(labels_multi)[test_index]\n","    train_masks, test_masks = np.array(attention_masks)[train_index], np.array(attention_masks)[test_index] \n","    \n","    model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=4).cuda()\n","\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'LayerNorm.weight']\n","\n","    optimizer_grouped_parameters = [\n","            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","            ]\n","\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=5e-6\n","\n","    # Inner loop\n","    for sub_train_index, dev_index in skf_inner.split(x_train, y_train):\n","        print('inner fold', fold_inner)\n","\n","        # Initialize previous dev loss\n","        previous_valid_loss = 1000\n","\n","        x_sub_train, x_dev = np.array(x_train)[sub_train_index], np.array(x_train)[dev_index]\n","        y_sub_train, y_dev = np.array(y_train)[sub_train_index], np.array(y_train)[dev_index]\n","        sub_train_masks, dev_masks = np.array(train_masks)[sub_train_index], np.array(train_masks)[dev_index] \n","\n","\n","        # Conver to longTensor\n","        x_sub_train = torch.LongTensor(x_sub_train)\n","        x_dev = torch.LongTensor(x_dev)\n","\n","        y_sub_train = torch.LongTensor(y_sub_train)\n","        y_dev = torch.LongTensor(y_dev)\n","\n","        sub_train_masks = torch.LongTensor(sub_train_masks)\n","        dev_masks = torch.LongTensor(dev_masks)\n","        \n","        # Pack to dataLoader\n","        train_data = TensorDataset(x_sub_train, sub_train_masks, y_sub_train)\n","        train_sampler = RandomSampler(train_data)\n","        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","            \n","        dev_data = TensorDataset(x_dev, dev_masks, y_dev)\n","        dev_sampler = RandomSampler(dev_data)\n","        dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)\n","\n","        # if it's not the first inner fold each outer fold, load the model to keep training\n","        if fold_count_inner%3 != 1:\n","          model.load_state_dict(torch.load('./stl_roberta.pkl'))\n","\n","        # Training \n","        for epoch in range(n_epoch):\n","            print(epoch)\n","\n","            model.train()\n","\n","            train_losses = []\n","            valid_losses = []\n","          \n","            for step, batch in enumerate(train_dataloader):\n","              # add batch to GPU\n","              batch = tuple(t.to(device) for t in batch)\n","\n","              # unpack the inputs from dataloader\n","              b_input_ids, b_input_mask, b_labels = batch \n","\n","              # clear out the gradients (by default they accumulate)\n","              optimizer.zero_grad()\n","\n","              # forward pass\n","              outputs = model(input_ids=b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","              loss = outputs.loss\n","\n","              # backward pass\n","              loss.backward()\n","\n","              # track train loss\n","              train_losses.append(loss.item())\n","\n","              # update parameters and take a step using the computed gradient\n","              optimizer.step()\n","\n","            train_loss = np.average(train_losses)\n","            print('train loss: {}'.format(train_loss))\n","    \n","            # Validation\n","            model.eval()\n","\n","            predictions = []\n","            targets = []\n","\n","            # evaluate data for one epoch\n","            for batch in dev_dataloader:\n","              # add batch to GPU\n","              batch = tuple(t.to(device) for t in batch)\n","\n","              # unpack the inputs from dataloader\n","              b_input_ids, b_input_mask, b_labels = batch\n","\n","              with torch.no_grad():\n","                # forward pass, calculate logit predictions\n","                outputs = model(input_ids=b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","                \n","                loss = outputs.loss\n","                logits = outputs.logits\n","\n","              valid_losses.append(loss.item())\n","\n","              # move logits and labels to CPU\n","              logits = logits.detach().cpu().numpy()\n","              labels = b_labels.to('cpu').numpy()\n","\n","              predictions = np.append(predictions, np.argmax(logits, axis=1))\n","              targets = np.append(targets, labels)\n","\n","            # Calculate dev loss and f1\n","            valid_loss = np.average(valid_losses)\n","            print('valid loss: {}'.format(valid_loss))\n","            dev_f1 = metrics.f1_score(targets, predictions, average='macro', zero_division=1)\n","            print(\"dev_f1:\", dev_f1)   \n"," \n","            # save the best model based on dev loss\n","            if valid_loss < previous_valid_loss:\n","\n","              previous_valid_loss = valid_loss\n","              if fold_count_inner%3 != 0:\n","                torch.save(model_roberta.state_dict(), './stl_roberta.pkl')\n","                \n","              else:\n","                torch.save(model_roberta, './stl_roberta.pkl')\n","\n","              print(\"saved\")\n","\n","        fold_inner += 1\n","        \n","    # Conver to longTensor           \n","    x_test = torch.LongTensor(x_test)\n","    y_test = torch.LongTensor(y_test)\n","    test_masks = torch.LongTensor(test_masks)\n","        \n","    # Pack to dataLoader\n","    test_data = TensorDataset(x_test, test_masks, y_test) \n","    test_sampler = SequentialSampler(test_data)\n","    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","    \n","    # Testing\n","    stl_roberta_model = torch.load('./stl_roberta.pkl')\n","    \n","    test_acc, test_precision, test_recall, test_f1 = testing(stl_roberta_model, test_dataloader)\n","\n","    test_acc_10.append(test_acc)\n","    test_precision_10.append(test_precision)\n","    test_recall_10.append(test_recall)\n","    test_f1_10.append(test_f1)\n","    \n","    fold_outer += 1\n","\n","print('end')\n","print(\"test_acc:\", np.average(test_acc_10))\n","print(\"test_precision:\", np.average(test_precision_10))\n","print(\"test_recall:\", np.average(test_recall_10))\n","print(\"test_f1:\", np.average(test_f1_10))      "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EW0BeOppySys"},"source":["## Testing"]},{"cell_type":"code","metadata":{"id":"MGCso-pwySLd"},"source":["def testing(stl_roberta_model, test_dataloader):\n","    \n","    test_predictions = []\n","    test_targets = []\n","    stl_roberta_model.eval()\n","    \n","    for batch in test_dataloader:\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        # Unpack the inputs from dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","       \n","        with torch.no_grad():\n","            # Forward pass, calculate logit predictions\n","            outputs = stl_roberta_model(input_ids=b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","            logits = outputs.logits\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        labels = b_labels.to('cpu').numpy()\n","\n","        test_predictions = np.append(test_predictions, np.argmax(logits, axis=1))\n","        test_targets = np.append(test_targets, labels)\n","    \n","    test_acc = metrics.accuracy_score(test_targets, test_predictions)\n","    test_precision = metrics.precision_score(test_targets, test_predictions, average=\"macro\", zero_division=1)\n","    test_recall = metrics.recall_score(test_targets, test_predictions, average=\"macro\", zero_division=1)\n","    test_f1 = metrics.f1_score(test_targets, test_predictions, average=\"macro\", zero_division=1)\n","    \n","    return test_acc, test_precision, test_recall, test_f1 "],"execution_count":null,"outputs":[]}]}