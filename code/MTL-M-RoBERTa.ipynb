{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MTL-M-RoBERTa.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMU+9PgnrbcCC7jWCABCDGE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"MqQloGTvWgIv"},"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold\n","import numpy as np\n","from keras.preprocessing.sequence import pad_sequences\n","from pytorch_pretrained_bert import BertAdam\n","from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaModel\n","from sklearn import metrics\n","import torch.nn as nn \n","from pytorch_pretrained_bert import modeling\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8LRQW4XMXnd6"},"source":["## Linguistic feature - Emotion\n","\n","complaints-svitlana.csv stores an array of each tweet represented by vectors ($N \\times 9$)"]},{"cell_type":"code","metadata":{"id":"JG3iMN6TXwg0"},"source":["df = pd.read_csv('./complaints-svitlana.csv', header=None)\n","features_linguistic = np.array(df.loc[:, 1:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UUeJyFw5XxjP"},"source":["## Preprocess and Prepare Dataset\n"]},{"cell_type":"code","metadata":{"id":"5bLIqL9EX0li"},"source":["# Set the maximum sequence length\n","MAX_LEN = 50\n","\n","df = pd.read_csv('./complaint_severity_data.csv', header=None, names=['id', 'text', 'binarylabel', 'multilabel', 'domain'])\n","\n","# Create sentence and label list\n","sentences = df.text.values\n","labels_binary = df.binarylabel.values\n","labels_multi = df.multilabel.values\n","\n","sentences = ['[CLS] ' + sentence + ' [SEP]' for sentence in sentences]\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","# Use the RoBERTa tokenizer to convert the tokens to their index numbers in the RoBERTa vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# Pad input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')\n","\n","# Create attention masks\n","attention_masks = []\n","\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mpZjCTzXYO-t"},"source":["## Attention gate \n","Control the influence of each representation"]},{"cell_type":"code","metadata":{"id":"LXhptTwxYRE6"},"source":["hidden_size = 768\n","embedding_size = 200\n","beta = 0.001\n","dropout_prob = 0.5\n","\n","class AttnGating(nn.Module):\n","  def __init__(self):\n","    super(AttnGating, self).__init__()\n","   \n","    self.linear = nn.Linear(9, embedding_size)\n","    self.relu = nn.ReLU(inplace=True)\n","\n","    self.weight_emotion_W1 = nn.Parameter(torch.Tensor(hidden_size+embedding_size, hidden_size))\n","    self.weight_emotion_W2 = nn.Parameter(torch.Tensor(embedding_size, hidden_size))\n"," \n","    \n","    nn.init.uniform_(self.weight_emotion_W1, -0.1, 0.1)\n","    nn.init.uniform_(self.weight_emotion_W2, -0.1, 0.1)\n","\n","    self.LayerNorm = nn.LayerNorm(hidden_size)\n","    self.dropout = nn.Dropout(dropout_prob)\n","\n","  def forward(self, embeddings_roberta, linguistic_feature):\n","     \n","     # Project linguistic representations into vectors with comparable size\n","     linguistic_feature = self.linear(linguistic_feature) \n","     emotion_feature = linguistic_feature.repeat(MAX_LEN, 1, 1) # (50, bs, 200) \n","     emotion_feature = emotion_feature.permute(1, 0, 2) # (bs, 50, 200)\n","\n","     # Concatnate word and linguistic representations  \n","     features_combine = torch.cat((emotion_feature, embeddings_roberta), axis=2) # (bs, 50, 968)\n","     \n","     g_feature = self.relu(torch.matmul(features_combine, self.weight_emotion_W1))\n","\n","     # Attention gating\n","     H = torch.mul(g_feature, torch.matmul(emotion_feature, self.weight_emotion_W2))\n","     alfa = min(beta * (torch.norm(embeddings_roberta)/torch.norm(H)), 1)\n","     E = torch.add(torch.mul(alfa, H), embeddings_roberta)\n","\n","     # Layer normalization and dropout \n","     embedding_output = self.dropout(self.LayerNorm(E)) \n","\n","     return embedding_output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDhNIRADY0WW"},"source":["## RoBERTa model"]},{"cell_type":"code","metadata":{"id":"RxyPckeiY6Eu"},"source":["class RobertaClassificationModel(nn.Module):\n","    def __init__(self):\n","        super(RobertaClassificationModel, self).__init__()\n","  \n","        self.roberta = RobertaModel.from_pretrained('roberta-base', add_pooling_layer=False, return_dict=True)\n","\n","        self.dropout = nn.Dropout(0.1)\n","        self.num_labels_complaint = 2\n","        self.num_labels_severity = 5\n","\n","        self.classifier_complaint = nn.Linear(768, 2)\n","        self.classifier_severity = nn.Linear(768, 5)\n","\n","        self.beta = 0.1\n","        \n","\n","    def forward(self, embedding_output, attention_mask, labels_complaint=None, labels_severity=None):\n"," \n","        outputs = self.roberta(input_ids=None, inputs_embeds=embedding_output, attention_mask=attention_mask)\n","        sequence_output  = outputs.last_hidden_state\n","\n","        x = sequence_output[:, 0, :]\n","        x = self.dropout(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","\n","        logits_complaint = self.classifier_complaint(x)\n","        logits_severity = self.classifier_severity(x)\n","        # Initialize loss of binary complaint\n","        loss_complaint = None\n","        \n","        # Training on binary complaint  \n","        if labels_complaint is not None:\n","          if self.num_labels_complaint == 1:\n","            loss_fct_complaint = nn.MSELoss()\n","            loss_complaint = loss_fct_complaint(logits_complaint.view(-1), labels_complaint.view(-1))\n","          else:\n","            loss_fct_complaint = nn.CrossEntropyLoss()\n","            loss_complaint = loss_fct_complaint(logits_complaint.view(-1, self.num_labels_complaint), labels_complaint.view(-1))\n","\n","        # Initialize loss of complaint severity \n","        loss_severity = None \n","\n","        # Training on complaint severity\n","        if labels_severity is not None:\n","          if self.num_labels_severity == 1:\n","            loss_fct_severity = nn.MSELoss()\n","            loss_severity = loss_fct_severity(logits_severity.view(-1), labels_severity.view(-1))\n","          else:\n","            loss_fct_severity = nn.CrossEntropyLoss()\n","            loss_severity = loss_fct_severity(logits_severity.view(-1, self.num_labels_severity), labels_severity.view(-1))\n","\n","        output = (logits_complaint,) + (logits_severity,) +outputs[2:]\n","\n","        loss = None\n","        \n","        # Total loss = (1-beta) * binary_complaint_loss + beta * complaint_severity_loss\n","        if labels_complaint is not None and labels_severity is not None:\n","          loss = (1-self.beta) * loss_complaint + self.beta * loss_severity\n","      \n","        return ((loss,) + output) if loss is not None else output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2i80mSJMemYf"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"kUsjO5zjeuVa"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"afxx8sdmeoAz"},"source":["batch_size = 32\n","n_epoch = 12\n","\n","# Nested cross validation (outer-10; inner-3)\n","skf_outer = StratifiedKFold(n_splits=10, random_state=100)\n","skf_inner = StratifiedKFold(n_splits=3, random_state=100)\n","  \n","fold_count_outer = 1\n","fold_count_inner =1\n","\n","# Acc/precision/recall/f1s of binary complaint classification over 10 folds\n","test_acc_10_c = []\n","test_precision_10_c = []\n","test_recall_10_c = []\n","test_f1_10_c = []\n","\n","# Acc/precision/recall/f1s of complaint severity classification over 10 folds\n","test_acc_10_s = []\n","test_precision_10_s = []\n","test_recall_10_s = []\n","test_f1_10_s = []\n","\n","# Outer loop\n","for train_index, test_index in skf_outer.split(input_ids, labels_binary):\n","    print('outter fold', fold_count_outer)\n","\n","    x_train, x_test = np.array(input_ids)[train_index], np.array(input_ids)[test_index]\n","    y_train, y_test = np.array(labels_binary)[train_index], np.array(labels_binary)[test_index]\n","    y_severity_train, y_severity_test = np.array(labels_multi)[train_index], np.array(labels_multi)[test_index]      \n","    train_masks, test_masks = np.array(attention_masks)[train_index], np.array(attention_masks)[test_index] \n","    train_features, test_features = np.array(features_linguistic)[train_index], np.array(features_linguistic)[test_index]    \n","\n","    attn_gate = AttnGating().cuda()\n","    embedding_roberta = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True).cuda()\n","    model_roberta = RobertaClassificationModel().cuda()\n","\n","    param_optimizer = list(model_roberta.named_parameters())\n","    no_decay = ['bias', 'LayerNorm.weight']        \n","        \n","    optimizer_grouped_parameters = [\n","            {'params': [p for n, p in model_roberta.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","            {'params': [p for n, p in model_roberta.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","            ]\n","\n","    optimizer = BertAdam(optimizer_grouped_parameters, lr=1e-6, warmup=.1)\n","\n","\n","    # Inner loop\n","    for sub_train_index, dev_index in skf_inner.split(x_train, y_train):\n","        print('inner fold', fold_count_inner)\n","\n","        # Initialize previous dev loss\n","        previous_valid_loss = 1000\n","\n","        x_sub_train, x_dev = np.array(x_train)[sub_train_index], np.array(x_train)[dev_index]\n","        y_sub_train, y_dev = np.array(y_train)[sub_train_index], np.array(y_train)[dev_index]\n","        y_sub_severity_train, y_dev_severity = np.array(y_severity_train)[sub_train_index], np.array(y_severity_train)[dev_index]\n","        sub_train_masks, dev_masks = np.array(train_masks)[sub_train_index], np.array(train_masks)[dev_index]\n","        sub_train_features, dev_features = np.array(train_features)[sub_train_index], np.array(train_features)[dev_index]\n","\n","        \n","        # Conver to longTensor\n","        x_sub_train = torch.LongTensor(x_sub_train)\n","        x_dev = torch.LongTensor(x_dev)\n","\n","        y_sub_train = torch.LongTensor(y_sub_train)\n","        y_dev = torch.LongTensor(y_dev)\n","\n","        y_sub_severity_train = torch.LongTensor(y_sub_severity_train)\n","        y_dev_severity = torch.LongTensor(y_dev_severity)\n","\n","        sub_train_masks = torch.LongTensor(sub_train_masks)\n","        dev_masks = torch.LongTensor(dev_masks)\n","\n","        sub_train_features = torch.FloatTensor(sub_train_features)\n","        dev_features = torch.FloatTensor(dev_features)\n","\n","\n","        # Pack to dataLoader\n","        train_data = TensorDataset(x_sub_train, sub_train_features, sub_train_masks, y_sub_train, y_sub_severity_train)\n","        train_sampler = RandomSampler(train_data)\n","        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","            \n","        dev_data = TensorDataset(x_dev, dev_features, dev_masks, y_dev, y_dev_severity)\n","        dev_sampler = RandomSampler(dev_data)\n","        dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)      \n","        \n","        \n","        # if it's not the first inner fold each outer fold, load the model to keep training\n","        if fold_count_inner%3 != 1:\n","          model_roberta.load_state_dict(torch.load('./mtl_mroberta.pkl'))\n","\n","        for epoch in range(n_epoch):\n","            print(epoch)\n","\n","            # Training\n","            model_roberta.train()\n","\n","            train_losses = []\n","            valid_losses = []\n","\n","            for step, batch in enumerate(train_dataloader):\n","                # Add batch to GPU\n","                batch = tuple(t.to(device) for t in batch)\n","\n","                # Unpack the inputs from dataloader\n","                b_input_ids, b_input_feature, b_input_mask, b_labels_binary, b_labels_multi = batch\n","  \n","                # Clear out the gradients (by default they accumulate)\n","                optimizer.zero_grad()\n","                \n","                # Generate combined representations\n","                last_hidden_state, pooler_output, all_hidden_states = embedding_roberta(b_input_ids) \n","                roberta_embed = all_hidden_states[0]\n","                combine_embed = attn_gate(roberta_embed, b_input_feature)\n","\n","                outputs = model_roberta(embedding_output=combine_embed, attention_mask=b_input_mask, labels_complaint=b_labels_binary, labels_severity=b_labels_multi)\n","                loss = outputs[0]\n","\n","                # Backward pass\n","                loss.backward()\n","\n","                # track train loss\n","                train_losses.append(loss.item())\n","\n","                # Update parameters and take a step using the computed gradient\n","                optimizer.step()\n","                \n","            train_loss = np.average(train_losses)\n","            print('train loss: {}'.format(train_loss))\n","\n","            # Validation\n","            model_roberta.eval()\n","\n","            predictions_complaint = []\n","            targets_complaint = []\n","\n","            predictions_severity = []\n","            targets_severity = []\n","\n","            # Evaluate data for one epoch\n","            for batch in dev_dataloader:\n","                # Add batch to GPU\n","                batch = tuple(t.to(device) for t in batch)\n","\n","                # Unpack the inputs from dataloader\n","                b_input_ids, b_input_feature, b_input_mask, b_labels_binary, b_labels_multi = batch\n","                \n","               \n","                with torch.no_grad():\n","\n","                    # Generate combined representations\n","                    last_hidden_state, pooler_output, all_hidden_states = embedding_roberta(b_input_ids) \n","                    roberta_embed = all_hidden_states[0]\n","                    combine_embed = attn_gate(roberta_embed, b_input_feature)\n","\n","                    outputs = model_roberta(embedding_output=combine_embed, attention_mask=b_input_mask, labels_complaint=b_labels_binary, labels_severity=b_labels_multi)\n","                    loss = outputs[0]\n","                    logits_complaint = outputs[1]\n","                    logits_severity = outputs[2]\n","                  \n","\n","                valid_losses.append(loss.item())\n","\n","                # Move logits and labels to CPU\n","                logits_complaint = logits_complaint.detach().cpu().numpy()\n","                logits_severity = logits_severity.detach().cpu().numpy()\n","\n","                labels_complaint = b_labels_binary.to('cpu').numpy() \n","                labels_severity = b_labels_multi.to('cpu').numpy() \n","\n","                predictions_complaint = np.append(predictions_complaint, np.argmax(logits_complaint, axis=1))\n","                targets_complaint = np.append(targets_complaint, labels_complaint) \n","\n","                predictions_severity = np.append(predictions_severity, np.argmax(logits_severity, axis=1))\n","                targets_severity = np.append(targets_severity, labels_severity)   \n","\n","            # Calculate total dev loss \n","            valid_loss = np.average(valid_losses)\n","            print('valid loss: {}'.format(valid_loss))\n","\n","            # Calculate dev f1 of binary complaint\n","            dev_f1_complaint = metrics.f1_score(targets_complaint, predictions_complaint, average='macro', zero_division=1)\n","            print(\"complaint dev_f1:\", dev_f1_complaint)\n","\n","            # Calculate dev f1 of comlaint severity\n","            dev_f1_severity = metrics.f1_score(targets_severity, predictions_severity, average='macro', zero_division=1)\n","            print(\"severity dev_f1:\", dev_f1_severity)\n","\n","            # Save the best model based on dev loss\n","            if valid_loss < previous_valid_loss:\n","              previous_valid_loss = valid_loss\n","              if fold_count_inner%3 != 0:\n","                torch.save(model_roberta.state_dict(), './mtl_mroberta.pkl')\n","                \n","              else:\n","                torch.save(model_roberta, './mtl_mroberta.pkl')\n","              print(\"saved\")  \n","        \n","\n","        fold_count_inner += 1\n","          \n","    \n","    # Conver to longTensor\n","    x_test = torch.LongTensor(x_test)\n","    y_test = torch.LongTensor(y_test) \n","    y_severity_test = torch.LongTensor(y_severity_test)\n","    test_masks = torch.LongTensor(test_masks)\n","    test_features = torch.FloatTensor(test_features)\n","\n","    # Pack to dataLoader\n","    test_data = TensorDataset(x_test, test_features, test_masks, y_test, y_severity_test) \n","    test_sampler = SequentialSampler(test_data)\n","    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","    \n","    # Testing\n","    complaint_model = torch.load('./mtl_mroberta.pkl')\n","\n","    test_acc_c, test_precision_c, test_recall_c, test_f1_c, test_acc_s, test_precision_s, test_recall_s, test_f1_s = testing(complaint_model, test_dataloader)\n","    \n","    test_acc_10_c.append(test_acc_c)\n","    test_precision_10_c.append(test_precision_c)\n","    test_recall_10_c.append(test_recall_c)\n","    test_f1_10_c.append(test_f1_c)\n","\n","    test_acc_10_s.append(test_acc_s)\n","    test_precision_10_s.append(test_precision_s)\n","    test_recall_10_s.append(test_recall_s)\n","    test_f1_10_s.append(test_f1_s)\n","\n","    fold_count_outer += 1\n","\n","\n","print('end')\n","\n","print('complaint')\n","print(\"test_acc:\", np.average(test_acc_10_c))\n","print(\"test_precision:\", np.average(test_precision_10_c))\n","print(\"test_recall:\", np.average(test_recall_10_c))\n","print(\"test_f1:\", np.average(test_f1_10_c))\n","\n","print('severity')\n","print(\"test_acc:\", np.average(test_acc_10_s))\n","print(\"test_precision:\", np.average(test_precision_10_s))\n","print(\"test_recall:\", np.average(test_recall_10_s))\n","print(\"test_f1:\", np.average(test_f1_10_s)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZGDD4PCXkycS"},"source":["## Testing"]},{"cell_type":"code","metadata":{"id":"Bcgo2cVok0lk"},"source":["def testing(complaint_model, test_dataloader):\n","    test_predictions_complaint = []\n","    test_targets_complaint = [] \n","\n","    test_predictions_severity = []\n","    test_targets_severity = [] \n","\n","    complaint_model.eval()\n","\n","    for batch in test_dataloader:\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","\n","        # Unpack the inputs from dataloader\n","        b_input_ids, b_input_feature, b_input_mask, b_labels_binary, b_labels_multi = batch\n","\n","        with torch.no_grad():\n"," \n","            last_hidden_state, pooler_output, all_hidden_states = embedding_roberta(b_input_ids) \n","            roberta_embed = all_hidden_states[0]\n","            combine_embed = attn_gate(roberta_embed, b_input_feature)\n","            \n","            outputs = complaint_model(embedding_output=combine_embed, attention_mask=b_input_mask)\n","            logits_complaint = outputs[0]\n","            logits_severity = outputs[1]\n","\n","        # Move logits and labels to CPU\n","\n","        logits_complaint = logits_complaint.detach().cpu().numpy()\n","        labels_complaint = b_labels_binary.to('cpu').numpy()\n","\n","        logits_severity = logits_severity.detach().cpu().numpy()\n","        labels_severity = b_labels_multi.to('cpu').numpy()\n","\n","        test_predictions_complaint = np.append(test_predictions_complaint, np.argmax(logits_complaint, axis=1))\n","        test_targets_complaint = np.append(test_targets_complaint, labels_complaint)\n","\n","        test_predictions_severity = np.append(test_predictions_severity, np.argmax(logits_severity, axis=1))\n","        test_targets_severity = np.append(test_targets_severity, labels_severity)\n","\n","    # test acc/precision/recall/f1 of binary complaint\n","    test_acc_c = metrics.accuracy_score(test_targets_complaint, test_predictions_complaint)\n","    test_precision_c = metrics.precision_score(test_targets_complaint, test_predictions_complaint, average=\"macro\", zero_division=1)\n","    test_recall_c = metrics.recall_score(test_targets_complaint, test_predictions_complaint, average=\"macro\", zero_division=1)\n","    test_f1_c = metrics.f1_score(test_targets_complaint, test_predictions_complaint, average=\"macro\", zero_division=1)\n","\n","    # test acc/precision/recall/f1 of complaint severity\n","    test_acc_s = metrics.accuracy_score(test_targets_severity, test_predictions_severity)\n","    test_precision_s = metrics.precision_score(test_targets_severity, test_predictions_severity, average=\"macro\", zero_division=1)\n","    test_recall_s = metrics.recall_score(test_targets_severity, test_predictions_severity, average=\"macro\", zero_division=1)\n","    test_f1_s = metrics.f1_score(test_targets_severity, test_predictions_severity, average=\"macro\", zero_division=1)\n","\n","    return test_acc_c, test_precision_c, test_recall_c, test_f1_c, test_acc_s, test_precision_s, test_recall_s, test_f1_s"],"execution_count":null,"outputs":[]}]}
