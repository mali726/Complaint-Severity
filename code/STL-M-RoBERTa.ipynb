{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"STL-M-RoBERTa.ipynb","provenance":[],"authorship_tag":"ABX9TyN4W4aSHebyqv0kI7G8/02K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"-Z_aK3RL8jHq"},"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","import pandas as pd\n","from sklearn.model_selection import StratifiedKFold\n","import numpy as np\n","from keras.preprocessing.sequence import pad_sequences\n","from transformers import AdamW\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel\n","from sklearn import metrics\n","import torch.nn as nn \n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LlNfH1Ah-Og1"},"source":["## Linguistic feature \n","Emotion, Topic, Emotion+Topic\n","\n","Giving Emotion for example:\n","\n","complaints-svitlana.csv stores an array of each tweet represented by vectors ($N \\times 9$)\n"]},{"cell_type":"code","metadata":{"id":"QBWnoTZc-YI-"},"source":["df = pd.read_csv('./complaints-svitlana.csv', header=None)\n","features_linguistic = np.array(df.loc[:, 1:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IezdtZHdVw2h"},"source":["## Preprocess and Prepare Dataset\n","complaint_severity_data_4label.csv is extract from complaint_severity_data.csv where multilabel = {1, 2, 3, 4} (excluding non-complaints)"]},{"cell_type":"code","metadata":{"id":"BBGz_LYZVvyY"},"source":["# Set the maximum sequence length\n","MAX_LEN = 50\n","\n","df = pd.read_csv('./complaint_severity_data_4label.csv', header=None, names=['id', 'text', 'multilabel', 'domain'])\n","\n","# Create sentence and label list\n","sentences = df.text.values\n","labels_multi = df.multilabel.values\n","\n","sentences = ['[CLS] ' + sentence + ' [SEP]' for sentence in sentences]\n","\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","\n","tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","# Use the RoBERTa tokenizer to convert the tokens to their index numbers in the RoBERTa vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","\n","# Pad input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype='long', truncating='post', padding='post')\n","\n","# Create attention masks\n","attention_masks = []\n","\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gr36NZPVWu8l"},"source":["## Attention gate \n","Control the influence of each representation"]},{"cell_type":"code","metadata":{"id":"T82yZR27XAhK"},"source":["hidden_size = 768\n","embedding_size = 200\n","beta = 0.001\n","dropout_prob = 0.5\n","\n","class AttnGating(nn.Module):\n","  def __init__(self):\n","    super(AttnGating, self).__init__()\n","   \n","    self.linear = nn.Linear(9, embedding_size)\n","    self.relu = nn.ReLU(inplace=True)\n","\n","    self.weight_emotion_W1 = nn.Parameter(torch.Tensor(hidden_size+embedding_size, hidden_size))\n","    self.weight_emotion_W2 = nn.Parameter(torch.Tensor(embedding_size, hidden_size))\n"," \n","    \n","    nn.init.uniform_(self.weight_emotion_W1, -0.1, 0.1)\n","    nn.init.uniform_(self.weight_emotion_W2, -0.1, 0.1)\n","\n","    self.LayerNorm = nn.LayerNorm(hidden_size)\n","    self.dropout = nn.Dropout(dropout_prob)\n","\n","  def forward(self, embeddings_roberta, linguistic_feature):\n","     \n","     # Project linguistic representations into vectors with comparable size\n","     linguistic_feature = self.linear(linguistic_feature) \n","     emotion_feature = linguistic_feature.repeat(MAX_LEN, 1, 1) # (50, bs, 200) \n","     emotion_feature = emotion_feature.permute(1, 0, 2) # (bs, 50, 200)\n","\n","     # Concatnate word and linguistic representations  \n","     features_combine = torch.cat((emotion_feature, embeddings_roberta), axis=2) # (bs, 49, 968)\n","     \n","     g_feature = self.relu(torch.matmul(features_combine, self.weight_emotion_W1))\n","\n","     # Attention gating\n","     H = torch.mul(g_feature, torch.matmul(emotion_feature, self.weight_emotion_W2))\n","     alfa = min(beta * (torch.norm(embeddings_roberta)/torch.norm(H)), 1)\n","     E = torch.add(torch.mul(alfa, H), embeddings_roberta)\n","\n","     # Layer normalization and dropout \n","     embedding_output = self.dropout(self.LayerNorm(E)) \n","\n","     return embedding_output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YfjQi-jUfrdG"},"source":["## Building model and training"]},{"cell_type":"code","metadata":{"id":"c2QFmMLRgXWL"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-T2ecONDfvfx"},"source":["batch_size = 32\n","n_epoch = 15\n","\n","# Nested cross validation (outer-10; inner-3)\n","skf_outer = StratifiedKFold(n_splits=10, random_state=100)\n","skf_inner = StratifiedKFold(n_splits=3, random_state=100)\n","\n","# Initialise outer and inner fold count   \n","fold_count_outer = 1\n","fold_count_inner = 1\n","\n","# Acc/precision/recall/f1s over 10 folds\n","test_acc_10 = []\n","test_precision_10 = []\n","test_recall_10 = []\n","test_f1_10 = []\n","\n","# Outer loop\n","for train_index, test_index in skf_outer.split(input_ids, labels_multi):\n","    print('outter fold', fold_count_outer)\n","\n","    x_train, x_test = np.array(input_ids)[train_index], np.array(input_ids)[test_index]\n","    y_train, y_test = np.array(labels_multi)[train_index], np.array(labels_multi)[test_index]\n","    train_masks, test_masks = np.array(attention_masks)[train_index], np.array(attention_masks)[test_index] \n","\n","    train_linguistic, test_linguistic = np.array(features_linguistic)[train_index], np.array(features_linguistic)[test_index]    \n","\n","\n","    attn_gate = AttnGating().cuda()\n","\n","    # RoBERTa embedding model - to combine with linguistic representations\n","    embedding_roberta = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True).cuda() \n","\n","    model_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=4, return_dict=True).cuda()\n","\n","    param_optimizer = list(model_roberta.named_parameters())\n","    no_decay = ['bias', 'LayerNorm.weight']        \n","        \n","    optimizer_grouped_parameters = [\n","            {'params': [p for n, p in model_roberta.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","            {'params': [p for n, p in model_roberta.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","            ]\n","\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=5e-6)\n","\n","    # Inner loop\n","    for sub_train_index, dev_index in skf_inner.split(x_train, y_train):\n","        print('inner fold', fold_count_inner)\n","\n","        # Initialize previous dev loss\n","        previous_valid_loss = 1000\n","\n","        x_sub_train, x_dev = np.array(x_train)[sub_train_index], np.array(x_train)[dev_index]\n","        y_sub_train, y_dev = np.array(y_train)[sub_train_index], np.array(y_train)[dev_index]\n","        sub_train_masks, dev_masks = np.array(train_masks)[sub_train_index], np.array(train_masks)[dev_index]\n","\n","        sub_train_linguistic, dev_linguistic = np.array(train_linguistic)[sub_train_index], np.array(train_linguistic)[dev_index]\n","\n","\n","        # Conver to longTensor\n","        x_sub_train = torch.LongTensor(x_sub_train)\n","        x_dev = torch.LongTensor(x_dev)\n","\n","        y_sub_train = torch.LongTensor(y_sub_train)\n","        y_dev = torch.LongTensor(y_dev)\n","\n","        sub_train_masks = torch.LongTensor(sub_train_masks)\n","        dev_masks = torch.LongTensor(dev_masks)\n","\n","        sub_train_linguistic = torch.FloatTensor(sub_train_linguistic)\n","        dev_linguistic = torch.FloatTensor(dev_linguistic)\n","\n","\n","        # Pack to dataLoader\n","        train_data = TensorDataset(x_sub_train, sub_train_linguistic, sub_train_masks, y_sub_train)\n","        train_sampler = RandomSampler(train_data)\n","        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","            \n","        dev_data = TensorDataset(x_dev, dev_linguistic, dev_masks, y_dev)\n","        dev_sampler = RandomSampler(dev_data)\n","        dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)      \n","        \n","\n","        # if it's not the first inner fold each outer fold, load the model to keep training\n","        if fold_count_inner%3 != 1:\n","          model_roberta.load_state_dict(torch.load('./roberta_severity.pkl'))\n","\n","        # Training\n","        for epoch in range(n_epoch):\n","            print(epoch)\n","          \n","            model_roberta.train()\n","\n","            train_losses = []\n","            valid_losses = []\n","\n","            for step, batch in enumerate(train_dataloader):\n","                # Add batch to GPU\n","                batch = tuple(t.to(device) for t in batch)\n","\n","                # Unpack the inputs from dataloader\n","                b_input_ids, b_input_linguistic, b_input_mask, b_labels = batch\n","  \n","                # Clear out the gradients (by default they accumulate)\n","                optimizer.zero_grad()\n","                \n","                # Generate combined representations\n","                last_hidden_state, pooler_output, all_hidden_states = embedding_roberta(b_input_ids) \n","                roberta_embed = all_hidden_states[0]\n","                combine_embed = attn_gate(roberta_embed, b_input_linguistic)\n","\n","                outputs = model_roberta(inputs_embeds=combine_embed, attention_mask=b_input_mask, labels=b_labels)\n","                loss = outputs.loss\n","                \n","                # Backward pass\n","                loss.backward()\n","\n","                # track train loss\n","                train_losses.append(loss.item())\n","\n","                # Update parameters and take a step using the computed gradient\n","                optimizer.step()\n","                \n","            train_loss = np.average(train_losses)\n","            print('train loss: {}'.format(train_loss))\n","\n","            # Validation\n","            model_roberta.eval()\n","\n","            predictions = []\n","            targets = []\n","\n","            # Evaluate data for one epoch\n","            for batch in dev_dataloader:\n","                # Add batch to GPU\n","                batch = tuple(t.to(device) for t in batch)\n","\n","                # Unpack the inputs from dataloader\n","                b_input_ids, b_input_linguistic, b_input_mask, b_labels = batch\n","               \n","                with torch.no_grad():\n","                    \n","                    # Generate combined representations\n","                    last_hidden_state, pooler_output, all_hidden_states = embedding_roberta(b_input_ids) \n","                    roberta_embed = all_hidden_states[0]\n","                    combine_embed = attn_gate(roberta_embed, b_input_linguistic)\n","\n","                    outputs = model_roberta(inputs_embeds=combine_embed, attention_mask=b_input_mask, labels=b_labels)\n","\n","                    loss = outputs.loss\n","                    logits = outputs.logits\n","                \n","                valid_losses.append(loss.item())\n","\n","                # Move logits and labels to CPU\n","                logits = logits.detach().cpu().numpy()\n","                labels= b_labels.to('cpu').numpy()\n","                \n","                predictions = np.append(predictions, np.argmax(logits, axis=1))\n","                targets = np.append(targets, labels)\n","                \n","            # Calculate dev loss and f1\n","            valid_loss = np.average(valid_losses)\n","            print('valid loss: {}'.format(valid_loss))\n","            dev_f1 = metrics.f1_score(targets, predictions, average='macro', zero_division=1)\n","            print(\"dev_f1:\", dev_f1)\n","\n","            # Save the best model based on dev loss\n","            if valid_loss < previous_valid_loss:\n","\n","              previous_valid_loss = valid_loss\n","              if fold_count_inner%3 != 0:\n","                torch.save(model_roberta.state_dict(), './roberta_severity.pkl')\n","                \n","              else:\n","                torch.save(model_roberta, './roberta_severity.pkl')\n","\n","              print(\"saved\")\n","\n","        fold_count_inner += 1         \n","    \n","    # Conver to longTensor\n","    x_test = torch.LongTensor(x_test)\n","    y_test = torch.LongTensor(y_test)\n","    test_masks = torch.LongTensor(test_masks)\n","    test_linguistic = torch.FloatTensor(test_linguistic)\n","\n","    # Pack to dataLoader\n","    test_data = TensorDataset(x_test, test_linguistic, test_masks, y_test) \n","    test_sampler = SequentialSampler(test_data)\n","    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","    \n","    # Testing\n","    severiy_model = torch.load('./roberta_severity.pkl')\n","\n","    test_acc, test_precision, test_recall, test_f1 = testing(severiy_model, test_dataloader)\n","\n","    test_acc_10.append(test_acc)\n","    test_precision_10.append(test_precision)\n","    test_recall_10.append(test_recall)\n","    test_f1_10.append(test_f1)\n","    \n","    fold_count_outer += 1\n","\n","print('end')\n","print(\"test_acc:\", np.average(test_acc_10))\n","print(\"test_precision:\", np.average(test_precision_10))\n","print(\"test_recall:\", np.average(test_recall_10))\n","print(\"test_f1:\", np.average(test_f1_10)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e--74pROfx0C"},"source":["## Testing"]},{"cell_type":"code","metadata":{"id":"4Xvqo7fufzB1"},"source":["def testing(severiy_model, test_dataloader):\n","    \n","    severiy_model.eval()\n","\n","    test_predictions = []\n","    test_targets = []  \n","    \n","    for batch in test_dataloader:\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","\n","        # Unpack the inputs from dataloader\n","        b_input_ids, b_input_linguistic, b_input_mask, b_labels = batch\n","        \n","        with torch.no_grad():\n","\n","            last_hidden_state, pooler_output, all_hidden_states = embedding_roberta(b_input_ids) \n","            roberta_embed = all_hidden_states[0]\n","            combine_embed = attn_gate(roberta_embed, b_input_linguistic)\n","\n","            outputs = severiy_model(inputs_embeds=combine_embed, attention_mask=b_input_mask)\n","            logits = outputs.logits\n","        \n","        # Move logits and labels to CPU\n","\n","        logits = logits.detach().cpu().numpy()\n","        labels = b_labels.to('cpu').numpy()\n","\n","        test_predictions = np.append(test_predictions, np.argmax(logits, axis=1))\n","        test_targets = np.append(test_targets, labels)\n","\n","    test_acc = metrics.accuracy_score(test_targets, test_predictions)\n","    test_precision = metrics.precision_score(test_targets, test_predictions, average=\"macro\", zero_division=1)\n","    test_recall = metrics.recall_score(test_targets, test_predictions, average=\"macro\", zero_division=1)\n","    test_f1 = metrics.f1_score(test_targets, test_predictions, average=\"macro\", zero_division=1)\n","\n","    return test_acc, test_precision, test_recall, test_f1 "],"execution_count":null,"outputs":[]}]}
